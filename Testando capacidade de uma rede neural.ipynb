{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare multiclass classification dataset\n",
    "\n",
    "def create_dataset():\n",
    "    #gerar um dataset 2d para classificação\n",
    "    X, y = make_blobs(n_samples = 1000, centers=20, n_features = 100, cluster_std=2,\n",
    "                     random_state = 2)\n",
    "    #one hot encode output\n",
    "    y = to_categorical(y)\n",
    "    \n",
    "    #dividir o dataset em treino e teste\n",
    "    n_train = 500\n",
    "    trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "    trainy, testy = y[:n_train, :], y[n_train:]\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treina o modelo com um dado numero de nós, retorna a accuracy para o conjunto de teste \n",
    "\n",
    "def evaluate_model(n_nodes, trainX, trainy, testX, testy):\n",
    "    #configura o modelo baseado nos dados\n",
    "    n_input, n_classes = trainX.shape[1], testy.shape[1]\n",
    "    #define a arquitetura do modelo\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_nodes, input_dim=n_input, activation='relu',\n",
    "                   kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    #compila o modelo\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrical=['accuracy'])\n",
    "    \n",
    "    #treina o modelo\n",
    "    history = model.fit(trainX, trainy, epochs=100, verbose=0)\n",
    "    \n",
    "    #avalia o model com dados de teste\n",
    "    _, test_acc = model.evalute(testX, testy, verbose=0)\n",
    "    return history, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avalia o modelo e plota a curva de aprendizagem\n",
    "num_nodes = [1,2,3,4,8,16,32,64]\n",
    "for n_nodes in num_nodes:\n",
    "    #avalia o modelo dado o numero de nos\n",
    "    history, result = evaluate_model(n_nodes, trainX, trainy, testX, testy)\n",
    "    \n",
    "    #sumariza a accuracy do teste\n",
    "    print('nodes=%d: %.3f' %(n_nodes, result))\n",
    "    #plota a curva de aprendizagem\n",
    "    pyplot.plot(history.history['loss'], label=str(n_nodes))\n",
    "#mostra o plot\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "def evaluate_model(n_nodes, trainX, trainy, testX, testy):\n",
    "    n_input, n_classes = trainX,,shape[0], testy,shape[0]\n",
    "    model = Sequential()\n",
    "    model = Sequential([tf.keras.layers.Flatten(),\n",
    "                       Dense(n_nodes, activation=tf.nn.relu),\n",
    "                       Dense(10, activation=tf.nn.softmax)])\n",
    "    model.compile(optmizer = tf.optimizers.Adam(I), loss = 'sparse_categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "    history = model.fit(training_images, training_labels, epochs = 5, verbose = 0)\n",
    "    _, test_acc = model.evaluate(test_images, test_labels)\n",
    "    return history, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = [16,32,64,128,256,512,1024,2048]\n",
    "\n",
    "for n_nodes in num_nodes:\n",
    "    history, result = evaluate_model(n_nodes, training_images,\n",
    "                                    training_labels, test_images, test_labels)\n",
    "    print('nodes=%d: %3f' % (n_nodes, result))\n",
    "    pyplot.plot(history.history['loss'], label = str(n_nodes))\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1* O jeito mais simples de evitar de o overfitting é reduzindo o tamanho do modelo, ou seja, reduzindo o numero de \n",
    "parametros (isso é determinado pela quantidade de camadas e neuronios em cada uma delas).\n",
    "\n",
    "Em deep learning isso é chamado de capacidade do modelo.\n",
    "\n",
    "Mais parametros significa maior capacidade de memorização e mais facilidade em mapear amostras de treino e seus targets, contudo perde poder de generalização, causando overfitting.\n",
    "\n",
    "Poucos parametros tambem são um problema, aumentando os erros das previsões, tornando o modelo sem utilidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2* Vamos investigar como o aumento do numero de nós(neuronios) pode impactar a capacidade de um modelo de redes neurais.\n",
    "\n",
    "Usaremos o sklearn.datasets.make.blobs para criar o dataset. Esse dataset terá 1000 amostras, com 100 features cada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3* Criamos uma função para avaliar o modelo, utilizamos uma rede neural com somente uma camada Densa.\n",
    "\n",
    "Essa função recebe os dados e numero de nós dessa unica camada e faz a avaliação de accuracy e loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4* Criamos uma função para avaliar o modelo, utilizamos uma rede neural com somente uma camada Densa\n",
    "\n",
    "O aumento do número de nós ajuda a reduzir o erro e aumentar a accuracy. Mas observer que acima de 16 nós temos overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5* Realizamos o mesme teste com dataset MNIST fashion, para detecção de objetos.\n",
    "\n",
    "Testamos com somente 5 épocas e com número maior de nós."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6* Observamos que há ganhos até o número de 256 neurônios.\n",
    "Após isso o resultado não melhora mais. Com essa análise podemos considerar que nã há vantagem em aumentar o número de nós para mais de 256.\n",
    "\n",
    "Deixaria o modelo mais lento para treinar e causaria overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não há jeito ideal de definir o número de neurônios.\n",
    "\n",
    "É necessário avaliar diferentes arquiteturas até encontrar o tamanho correti do modelo para o seu dado.\n",
    "\n",
    "Modificar o número de nós é buscar a \"largura\" ideal da rede neural.\n",
    "\n",
    "O número de camadas é chamada de \"profundidade\" da rede.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
